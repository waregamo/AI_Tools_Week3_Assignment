{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e23f2c-716e-4f3b-8e74-61ea703ef719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before cleaning:\n",
      "sepal length (cm)    1\n",
      "sepal width (cm)     0\n",
      "petal length (cm)    0\n",
      "petal width (cm)     0\n",
      "species              0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after cleaning:\n",
      "sepal length (cm)    0\n",
      "sepal width (cm)     0\n",
      "petal length (cm)    0\n",
      "petal width (cm)     0\n",
      "species              0\n",
      "dtype: int64\n",
      "\n",
      "Model Evaluation:\n",
      "Accuracy: 0.9\n",
      "Precision: 0.9285714285714285\n",
      "Recall: 0.8888888888888888\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.67      0.80         9\n",
      "           2       0.79      1.00      0.88        11\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.93      0.89      0.89        30\n",
      "weighted avg       0.92      0.90      0.90        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1: Import the Necessary Libraries\n",
    "\n",
    "import pandas as pd                  \n",
    "import numpy as np                    \n",
    "from sklearn.datasets import load_iris \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.tree import DecisionTreeClassifier         \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# 2: Load the Iris Dataset\n",
    "\n",
    "# The load_iris() function returns a dictionary-like object that contains data and target labels.\n",
    "iris = load_iris()\n",
    "\n",
    "# The iris.data contains the measurements like sepal length, sepal width, petal length, and petal width.\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "df['species'] = iris.target\n",
    "\n",
    "\n",
    "# 3: Data Preprocessing\n",
    "\n",
    "#Introduce a missing value to simulate incomplete data.\n",
    "df.loc[0, 'sepal length (cm)'] = np.nan\n",
    "\n",
    "# Check for missing values in each column.\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Remove rows that have any missing values using the dropna() method.\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Check again for missing values to ensure they have been removed.\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "# 4: Split the Data into Training and Testing Sets\n",
    "# Separate the data into features (X) and labels (y).\n",
    "X = df.drop(columns=['species'])  # \n",
    "y = df['species']                 \n",
    "\n",
    "# Split the data into training data and testing data.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "#5: Train the Decision Tree Classifier\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier.\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the decision tree model using the training data.\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#6 Make Predictions and Evaluate the Model\n",
    "\n",
    "# Use the trained model to predict the species labels for the test data.\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of our predictions.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate precision and recall. We use 'macro' to average metrics for all classes equally.\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Print the evaluation metrics.\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# For a more detailed summary (including F1-score for each class), print a classification report.\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9fc-5b47-4c52-bee6-89daf530ccf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2024.02-py310",
   "language": "python",
   "name": "conda-env-anaconda-2024.02-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
